{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'It is a good practice for us.'\n",
    "sent2 = 'It was also good to know about it.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = {}\n",
    "\n",
    "# Tokenize the sentences\n",
    "\n",
    "tokens1 = word_tokenize(sent1.lower())\n",
    "tokens2 = word_tokenize(sent2.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens1 = list(map(str.lower, tokens1))\n",
    "# tokens2 = list(map(str.lower, tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'practice',\n",
       " 'for',\n",
       " 'us',\n",
       " '.',\n",
       " 'it',\n",
       " 'was',\n",
       " 'also',\n",
       " 'good',\n",
       " 'to',\n",
       " 'know',\n",
       " 'about',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokens1 + tokens2\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['also',\n",
       " 'about',\n",
       " 'a',\n",
       " 'us',\n",
       " 'is',\n",
       " 'for',\n",
       " 'practice',\n",
       " 'it',\n",
       " 'to',\n",
       " 'know',\n",
       " 'was',\n",
       " '.',\n",
       " 'good']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = list(set(tokens))\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>about</th>\n",
       "      <th>a</th>\n",
       "      <th>us</th>\n",
       "      <th>is</th>\n",
       "      <th>for</th>\n",
       "      <th>practice</th>\n",
       "      <th>it</th>\n",
       "      <th>to</th>\n",
       "      <th>know</th>\n",
       "      <th>was</th>\n",
       "      <th>.</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  also about    a   us   is  for practice   it   to know  was    . good\n",
       "1  NaN   NaN  NaN  NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "2  NaN   NaN  NaN  NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df = pd.DataFrame(index=[1,2], columns=bow)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts1 = [tokens1.count(word) for word in bow_df.columns] # count words in sentence 1\n",
    "counts2 = [tokens2.count(word) for word in bow_df.columns] # count word for sentence 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_df.iloc[0,:] = counts1 # filling the first row with counts1\n",
    "bow_df.iloc[1,:] = counts2 # filling the second row with count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>about</th>\n",
       "      <th>a</th>\n",
       "      <th>us</th>\n",
       "      <th>is</th>\n",
       "      <th>for</th>\n",
       "      <th>practice</th>\n",
       "      <th>it</th>\n",
       "      <th>to</th>\n",
       "      <th>know</th>\n",
       "      <th>was</th>\n",
       "      <th>.</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  also about  a us is for practice it to know was  . good\n",
       "1    0     0  1  1  1   1        1  1  0    0   0  1    1\n",
       "2    1     1  0  0  0   0        0  2  1    1   1  1    1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>about</th>\n",
       "      <th>a</th>\n",
       "      <th>us</th>\n",
       "      <th>is</th>\n",
       "      <th>for</th>\n",
       "      <th>practice</th>\n",
       "      <th>it</th>\n",
       "      <th>to</th>\n",
       "      <th>know</th>\n",
       "      <th>was</th>\n",
       "      <th>.</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  also about    a   us   is  for practice   it   to know  was    . good\n",
       "1  NaN   NaN  NaN  NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
       "2  NaN   NaN  NaN  NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df1 = pd.DataFrame(index=[1,2], columns=bow)\n",
    "bow_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dai\\AppData\\Local\\Temp\\ipykernel_24548\\4122906072.py:1: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  bow_df1.fillna(0, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>about</th>\n",
       "      <th>a</th>\n",
       "      <th>us</th>\n",
       "      <th>is</th>\n",
       "      <th>for</th>\n",
       "      <th>practice</th>\n",
       "      <th>it</th>\n",
       "      <th>to</th>\n",
       "      <th>know</th>\n",
       "      <th>was</th>\n",
       "      <th>.</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   also  about  a  us  is  for  practice  it  to  know  was  .  good\n",
       "1     0      0  0   0   0    0         0   0   0     0    0  0     0\n",
       "2     0      0  0   0   0    0         0   0   0     0    0  0     0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df1.fillna(0, inplace=True)\n",
    "bow_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'it': 1,\n",
       "         'is': 1,\n",
       "         'a': 1,\n",
       "         'good': 1,\n",
       "         'practice': 1,\n",
       "         'for': 1,\n",
       "         'us': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counted1 = Counter(tokens1)\n",
    "counted1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'it': 2,\n",
       "         'was': 1,\n",
       "         'also': 1,\n",
       "         'good': 1,\n",
       "         'to': 1,\n",
       "         'know': 1,\n",
       "         'about': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counted2 = Counter(tokens2)\n",
    "counted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>about</th>\n",
       "      <th>a</th>\n",
       "      <th>us</th>\n",
       "      <th>is</th>\n",
       "      <th>for</th>\n",
       "      <th>practice</th>\n",
       "      <th>it</th>\n",
       "      <th>to</th>\n",
       "      <th>know</th>\n",
       "      <th>was</th>\n",
       "      <th>.</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   also  about  a  us  is  for  practice  it  to  know  was  .  good\n",
       "1     0      0  1   1   1    1         1   1   0     0    0  1     1\n",
       "2     0      0  0   0   0    0         0   0   0     0    0  0     0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in counted1:\n",
    "    bow_df1.loc[1,k] += counted1[k]\n",
    "\n",
    "bow_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>about</th>\n",
       "      <th>a</th>\n",
       "      <th>us</th>\n",
       "      <th>is</th>\n",
       "      <th>for</th>\n",
       "      <th>practice</th>\n",
       "      <th>it</th>\n",
       "      <th>to</th>\n",
       "      <th>know</th>\n",
       "      <th>was</th>\n",
       "      <th>.</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   also  about  a  us  is  for  practice  it  to  know  was  .  good\n",
       "1     0      0  1   1   1    1         1   1   0     0    0  1     1\n",
       "2     1      1  0   0   0    0         0   2   1     1    1  1     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k in counted2:\n",
    "    bow_df1.loc[2,k] += counted2[k]\n",
    "\n",
    "bow_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using built in library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate CountVectorizer\n",
    "cvt = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = cvt.fit_transform([sent1, sent2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0],\n",
       "       [1, 1, 0, 1, 0, 2, 1, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['about', 'also', 'for', 'good', 'is', 'it', 'know', 'practice',\n",
       "       'to', 'us', 'was'], dtype=object)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvt.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>also</th>\n",
       "      <th>for</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>know</th>\n",
       "      <th>practice</th>\n",
       "      <th>to</th>\n",
       "      <th>us</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  also  for  good  is  it  know  practice  to  us  was\n",
       "0      0     0    1     1   1   1     0         1   0   1    0\n",
       "1      1     1    0     1   0   2     1         0   1   0    1"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a dataframe with the data from cvt.fit_transform and columns as unique names\n",
    "df = pd.DataFrame(data=x_new.toarray(), columns=cvt.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = 'It was good for us.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_features = cvt.transform([new])\n",
    "new_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class CountVectorizer(_VectorizerMixin, sklearn.base.BaseEstimator)\n",
      " |  CountVectorizer(*, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |\n",
      " |  Convert a collection of text documents to a matrix of token counts.\n",
      " |\n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.csr_matrix.\n",
      " |\n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |\n",
      " |  For an efficiency comparison of the different feature extractors, see\n",
      " |  :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n",
      " |\n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : {'filename', 'file', 'content'}, default='content'\n",
      " |      - If `'filename'`, the sequence passed as an argument to fit is\n",
      " |        expected to be a list of filenames that need reading to fetch\n",
      " |        the raw content to analyze.\n",
      " |\n",
      " |      - If `'file'`, the sequence items must have a 'read' method (file-like\n",
      " |        object) that is called to fetch the bytes in memory.\n",
      " |\n",
      " |      - If `'content'`, the input is expected to be a sequence of items that\n",
      " |        can be of type string or byte.\n",
      " |\n",
      " |  encoding : str, default='utf-8'\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |\n",
      " |  decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |\n",
      " |  strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
      " |      Remove accents and perform other character normalization\n",
      " |      during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      a direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) means no character normalization is performed.\n",
      " |\n",
      " |      Both 'ascii' and 'unicode' use NFKD normalization from\n",
      " |      :func:`unicodedata.normalize`.\n",
      " |\n",
      " |  lowercase : bool, default=True\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |\n",
      " |  preprocessor : callable, default=None\n",
      " |      Override the preprocessing (strip_accents and lowercase) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer`` is not callable.\n",
      " |\n",
      " |  tokenizer : callable, default=None\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |\n",
      " |  stop_words : {'english'}, list, default=None\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |      There are several known issues with 'english' and you should\n",
      " |      consider an alternative (see :ref:`stop_words`).\n",
      " |\n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |\n",
      " |      If None, no stop words will be used. In this case, setting `max_df`\n",
      " |      to a higher value, such as in the range (0.7, 1.0), can automatically detect\n",
      " |      and filter stop words based on intra corpus document frequency of terms.\n",
      " |\n",
      " |  token_pattern : str or None, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |\n",
      " |      If there is a capturing group in token_pattern then the\n",
      " |      captured group content, not the entire match, becomes the token.\n",
      " |      At most one capturing group is permitted.\n",
      " |\n",
      " |  ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      word n-grams or char n-grams to be extracted. All values of n such\n",
      " |      such that min_n <= n <= max_n will be used. For example an\n",
      " |      ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
      " |      unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
      " |      Only applies if ``analyzer`` is not callable.\n",
      " |\n",
      " |  analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      " |      Whether the feature should be made of word n-gram or character\n",
      " |      n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |\n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |\n",
      " |      .. versionchanged:: 0.21\n",
      " |\n",
      " |      Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
      " |      first read from the file and then passed to the given callable\n",
      " |      analyzer.\n",
      " |\n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |\n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |\n",
      " |  max_features : int, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      `max_features` ordered by term frequency across the corpus.\n",
      " |      Otherwise, all features are used.\n",
      " |\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |\n",
      " |  vocabulary : Mapping or iterable, default=None\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |\n",
      " |  binary : bool, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |\n",
      " |  dtype : dtype, default=np.int64\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |\n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |\n",
      " |  fixed_vocabulary_ : bool\n",
      " |      True if a fixed vocabulary of term to indices mapping\n",
      " |      is provided by the user.\n",
      " |\n",
      " |  See Also\n",
      " |  --------\n",
      " |  HashingVectorizer : Convert a collection of text documents to a\n",
      " |      matrix of token counts.\n",
      " |\n",
      " |  TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
      " |      of TF-IDF features.\n",
      " |\n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.feature_extraction.text import CountVectorizer\n",
      " |  >>> corpus = [\n",
      " |  ...     'This is the first document.',\n",
      " |  ...     'This document is the second document.',\n",
      " |  ...     'And this is the third one.',\n",
      " |  ...     'Is this the first document?',\n",
      " |  ... ]\n",
      " |  >>> vectorizer = CountVectorizer()\n",
      " |  >>> X = vectorizer.fit_transform(corpus)\n",
      " |  >>> vectorizer.get_feature_names_out()\n",
      " |  array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
      " |         'this'], ...)\n",
      " |  >>> print(X.toarray())\n",
      " |  [[0 1 1 1 0 0 1 0 1]\n",
      " |   [0 2 0 1 0 1 1 0 1]\n",
      " |   [1 0 0 1 1 0 1 1 1]\n",
      " |   [0 1 1 1 0 0 1 0 1]]\n",
      " |  >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
      " |  >>> X2 = vectorizer2.fit_transform(corpus)\n",
      " |  >>> vectorizer2.get_feature_names_out()\n",
      " |  array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
      " |         'second document', 'the first', 'the second', 'the third', 'third one',\n",
      " |         'this document', 'this is', 'this the'], ...)\n",
      " |   >>> print(X2.toarray())\n",
      " |   [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " |   [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " |   [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " |   [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      _VectorizerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, *, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which generates either str, unicode or file objects.\n",
      " |\n",
      " |      y : None\n",
      " |          This parameter is ignored.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted vectorizer.\n",
      " |\n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return document-term matrix.\n",
      " |\n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which generates either str, unicode or file objects.\n",
      " |\n",
      " |      y : None\n",
      " |          This parameter is ignored.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |\n",
      " |  get_feature_names_out(self, input_features=None)\n",
      " |      Get output feature names for transformation.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : array-like of str or None, default=None\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names_out : ndarray of str objects\n",
      " |          Transformed feature names.\n",
      " |\n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays of shape (n_samples,)\n",
      " |          List of arrays of terms.\n",
      " |\n",
      " |  set_fit_request(self: sklearn.feature_extraction.text.CountVectorizer, *, raw_documents: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.feature_extraction.text.CountVectorizer from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |\n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      The options for each parameter are:\n",
      " |\n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |\n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |\n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |\n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |\n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |\n",
      " |      .. versionadded:: 1.3\n",
      " |\n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``raw_documents`` parameter in ``fit``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |\n",
      " |  set_transform_request(self: sklearn.feature_extraction.text.CountVectorizer, *, raw_documents: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.feature_extraction.text.CountVectorizer from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
      " |      Request metadata passed to the ``transform`` method.\n",
      " |\n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      The options for each parameter are:\n",
      " |\n",
      " |      - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided.\n",
      " |\n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``.\n",
      " |\n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |\n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |\n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |\n",
      " |      .. versionadded:: 1.3\n",
      " |\n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``raw_documents`` parameter in ``transform``.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |\n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |\n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which generates either str, unicode or file objects.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix of shape (n_samples, n_features)\n",
      " |          Document-term matrix.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _VectorizerMixin:\n",
      " |\n",
      " |  build_analyzer(self)\n",
      " |      Return a callable to process input data.\n",
      " |\n",
      " |      The callable handles preprocessing, tokenization, and n-grams generation.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      analyzer: callable\n",
      " |          A function to handle preprocessing, tokenization\n",
      " |          and n-grams generation.\n",
      " |\n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      preprocessor: callable\n",
      " |            A function to preprocess the text before tokenization.\n",
      " |\n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      tokenizer: callable\n",
      " |            A function to split a string into a sequence of tokens.\n",
      " |\n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols.\n",
      " |\n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc : bytes or str\n",
      " |          The string to decode.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      doc: str\n",
      " |          A string of unicode symbols.\n",
      " |\n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      stop_words: list or None\n",
      " |              A list of stop words.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _VectorizerMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  __sklearn_clone__(self)\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |\n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |\n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |\n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |\n",
      " |  __init_subclass__(**kwargs)\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |\n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |\n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |\n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram df using CountVectorizer for counting tokens with the given range of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = CountVectorizer(ngram_range=(1,2)) # one word frequency or 2 word frequency per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ng = ngram.fit_transform([sent1, sent2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = pd.DataFrame(data=new_ng.toarray(), columns=ngram.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>about it</th>\n",
       "      <th>also</th>\n",
       "      <th>also good</th>\n",
       "      <th>for</th>\n",
       "      <th>for us</th>\n",
       "      <th>good</th>\n",
       "      <th>good practice</th>\n",
       "      <th>good to</th>\n",
       "      <th>is</th>\n",
       "      <th>...</th>\n",
       "      <th>it was</th>\n",
       "      <th>know</th>\n",
       "      <th>know about</th>\n",
       "      <th>practice</th>\n",
       "      <th>practice for</th>\n",
       "      <th>to</th>\n",
       "      <th>to know</th>\n",
       "      <th>us</th>\n",
       "      <th>was</th>\n",
       "      <th>was also</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   about  about it  also  also good  for  for us  good  good practice  \\\n",
       "0      0         0     0          0    1       1     1              1   \n",
       "1      1         1     1          1    0       0     1              0   \n",
       "\n",
       "   good to  is  ...  it was  know  know about  practice  practice for  to  \\\n",
       "0        0   1  ...       0     0           0         1             1   0   \n",
       "1        1   0  ...       1     1           1         0             0   1   \n",
       "\n",
       "   to know  us  was  was also  \n",
       "0        0   1    0         0  \n",
       "1        1   0    1         1  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = CountVectorizer(ngram_range=(2,2))\n",
    "new_ng = ngram.fit_transform([sent1, sent2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about it</th>\n",
       "      <th>also good</th>\n",
       "      <th>for us</th>\n",
       "      <th>good practice</th>\n",
       "      <th>good to</th>\n",
       "      <th>is good</th>\n",
       "      <th>it is</th>\n",
       "      <th>it was</th>\n",
       "      <th>know about</th>\n",
       "      <th>practice for</th>\n",
       "      <th>to know</th>\n",
       "      <th>was also</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   about it  also good  for us  good practice  good to  is good  it is  \\\n",
       "0         0          0       1              1        0        1      1   \n",
       "1         1          1       0              0        1        0      0   \n",
       "\n",
       "   it was  know about  practice for  to know  was also  \n",
       "0       0           0             1        0         0  \n",
       "1       1           1             0        1         1  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf = pd.DataFrame(data=new_ng.toarray(), columns=ngram.get_feature_names_out())\n",
    "ndf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLNov2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
